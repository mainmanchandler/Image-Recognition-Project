{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CP467: Course Project\n",
    "Chandler Mayberry (190688910) - Samson Goodenough (190723380)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import cv2\n",
    "from cv2 import Stitcher\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Objects\n",
    "Load all object images and assign them names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJECT_NAMES = {\n",
    "  \"O1.png\": \"Controller\",\n",
    "  \"O2.png\": \"Chicken Noodle Soup\",\n",
    "  \"O3.png\": \"Adobo\",\n",
    "  \"O4.png\": \"Diet Coke\",\n",
    "  \"O5.png\": \"Mug\",\n",
    "  \"O6.png\": \"Skull\",\n",
    "  \"O7.png\": \"Cleaner\",\n",
    "  \"O8.png\": \"Remote\",\n",
    "  \"O9.png\": \"Lapras\",\n",
    "  \"O10.png\": \"Weight\",\n",
    "  \"O11.png\": \"Figure\",\n",
    "  \"O12.png\": \"Lanyard\",\n",
    "  \"O13.png\": \"Pokemon Card\",\n",
    "  \"O14.png\": \"Record\",\n",
    "  \"O15.png\": \"Kraft Dinner\"\n",
    "}\n",
    "\n",
    "objects = []\n",
    "\n",
    "# get all of the images in the directory \n",
    "for file in os.listdir(\"../Objects/\"):\n",
    "  img = cv2.imread(\"../Objects/\" + file)\n",
    "  fixed = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "  objects.append({\"name\": OBJECT_NAMES[file], \"image\": fixed, \"id\": file[:-4]})\n",
    "\n",
    "\n",
    "# show the object images\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(len(objects)):\n",
    "  # plot with title and no axis\n",
    "  plt.subplot(1, len(objects), i+1)\n",
    "  plt.imshow(objects[i][\"image\"])\n",
    "  plt.axis(\"off\")\n",
    "  plt.title(objects[i][\"name\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Scenes\n",
    "Load all scene images and group them into the different stitchable scenes.\n",
    "\n",
    "| Group # | Scenes    |\n",
    "|---------|-----------|\n",
    "| Group 1 | S1 - S6   |\n",
    "| Group 2 | S7        |\n",
    "| Group 3 | S8 - S14  |\n",
    "| Group 4 | S15 - S19 |\n",
    "| Group 5 | S20 - S21 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes = {}\n",
    "groups = {\n",
    "  \"group_1\": [],\n",
    "  \"group_2\": [],\n",
    "  \"group_3\": [],\n",
    "  \"group_4\": [],\n",
    "  \"group_5\": []\n",
    "}\n",
    "GROUP_LOOKUP = {\n",
    "  \"group_1\": [\"S1.png\", \"S2.png\", \"S3.png\", \"S4.png\", \"S5.png\", \"S6.png\"],\n",
    "  \"group_2\": [\"S7.png\"],\n",
    "  \"group_3\": [\"S8.png\", \"S9.png\", \"S10.png\", \"S11.png\", \"S12.png\"],\n",
    "  \"group_4\": [\"S15.png\", \"S16.png\", \"S17.png\", \"S18.png\", \"S19.png\"],\n",
    "  \"group_5\": [\"S20.png\", \"S21.png\"]\n",
    "}\n",
    "\n",
    "# use os to get all of the files in the directory\n",
    "for file in os.listdir(\"../Scenes/\"):\n",
    "  img = cv2.imread(\"../Scenes/\" + file)\n",
    "  fixed = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "  scenes[file] = fixed\n",
    "  for key in GROUP_LOOKUP:\n",
    "    if file in GROUP_LOOKUP[key]:\n",
    "      groups[key].append(fixed)\n",
    "\n",
    "# show the scene images\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, key in enumerate(scenes):\n",
    "  # plot with title and no axis\n",
    "  plt.subplot(1, len(scenes), i+1)\n",
    "  plt.imshow(scenes[key])\n",
    "  plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running All Tests ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summarize_images(key, scene, objects, object_keypoints, object_descriptors, min_matches = 28, threshold = 0.7):\n",
    "\n",
    "    #The boxed and scenes containing all of the keypoints need to survive on each iteration:\n",
    "    boxes_output_scene = scene.copy()\n",
    "    keypoints_output_scene = scene.copy()\n",
    "\n",
    "    for i, curr_obj in enumerate(objects):\n",
    "        raw_scene = scene.copy() #for the matching output (need raw scene everytime to match individual objects)\n",
    "        sift_instance = cv2.SIFT_create()\n",
    "        \n",
    "        obj_keypoints = object_keypoints[i]\n",
    "        obj_descriptors = object_descriptors[i]\n",
    "        scene_keypoints, scene_descriptors = sift_instance.detectAndCompute(raw_scene, None)\n",
    "        \n",
    "        #https://docs.opencv.org/3.4/dc/dc3/tutorial_py_matcher.html\n",
    "        brute_force_matcher = cv2.BFMatcher()\n",
    "        matches = brute_force_matcher.knnMatch(obj_descriptors, scene_descriptors, k=2)\n",
    "\n",
    "        #Check to see what matches out of them are within the predefined threshold (overall closeness)\n",
    "        close_matches = []\n",
    "        close_matches_nonlist = []\n",
    "\n",
    "        for m, n in matches:\n",
    "            if m.distance < threshold*n.distance:\n",
    "                close_matches.append([m])\n",
    "                close_matches_nonlist.append(m)\n",
    "\n",
    "        if len(close_matches) >= min_matches:\n",
    "             \n",
    "            #decrease the total number of matches to optimize the runtime below\n",
    "            close_matches = close_matches[:80]\n",
    "            close_matches_nonlist = close_matches_nonlist[:80]\n",
    "\n",
    "            boxes_output_scene = boxes(boxes_output_scene, curr_obj, obj_keypoints, scene_keypoints, close_matches_nonlist)\n",
    "            keypoints_output_scene = all_keypoints_on_scene(keypoints_output_scene, scene_keypoints, close_matches)\n",
    "            matching(key, raw_scene, curr_obj, scene_keypoints, close_matches)            \n",
    "\n",
    "\n",
    "    #this write is for the output with boxes + color fixing\n",
    "    boxes_output_scene = cv2.cvtColor(boxes_output_scene, cv2.COLOR_BGR2RGB) \n",
    "    cv2.imwrite(f\"../Detected_Objects/{key}_detected.png\", boxes_output_scene)\n",
    "\n",
    "    #this write is for the output with all keypoints for scene + color fixing\n",
    "    keypoints_output_scene = cv2.cvtColor(keypoints_output_scene, cv2.COLOR_BGR2RGB) \n",
    "    cv2.imwrite(f\"../Keypoints/{key}_keypoints.png\", keypoints_output_scene)\n",
    "    \n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Shows each object detected in the scene in a labelled box.\n",
    "\"\"\"\n",
    "#here we want to return the scene as we want to write over it on every iteration\n",
    "def boxes(boxes_output_scene, curr_obj, obj_keypoints, scene_keypoints, close_matches_nonlist):\n",
    "\n",
    "    # https://docs.opencv.org/3.4/d1/de0/tutorial_py_feature_homography.html\n",
    "\n",
    "    curr_obj_image = curr_obj[\"image\"]\n",
    "\n",
    "    # draw box around detected image\n",
    "    object_points = np.float32([ obj_keypoints[m.queryIdx].pt for m in close_matches_nonlist ]).reshape(-1, 1, 2)\n",
    "    scene_points = np.float32([ scene_keypoints[m.trainIdx].pt for m in close_matches_nonlist ]).reshape(-1, 1, 2)\n",
    "    mask, _ = cv2.findHomography(object_points, scene_points, cv2.RANSAC, 5.0)\n",
    "    height, width, _ = curr_obj_image.shape\n",
    "    points = np.float32([ [0, 0], [0, height-1], [width-1, height-1], [width-1, 0] ]).reshape(-1, 1, 2)\n",
    "    scene = cv2.perspectiveTransform(points, mask)\n",
    "    boxes_output_scene = cv2.polylines(boxes_output_scene,[np.int32(scene)], True, 255, 3, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "    # label all of the scene objects\n",
    "    # https://docs.opencv.org/4.x/d6/d6e/group__imgproc__draw.html#ga5126f47f883d730f633d74f07456c576\n",
    "    cv2.putText(img=boxes_output_scene, text=curr_obj['name'], org=(int(scene[0][0][0]), int(scene[1][0][1])), fontFace=cv2.FONT_HERSHEY_DUPLEX, fontScale=3, color=(0, 0, 0), thickness=4, lineType=cv2.FILLED, bottomLeftOrigin = False)\n",
    "    \n",
    "    return boxes_output_scene\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Gets all of the keypoints from all objects in each scene and for each objects and saves each object/scene individually with keypoints shown.\n",
    "\"\"\"\n",
    "#this we want to be returning the scene as we want to write the keypoints of all of the objects onto the single scene\n",
    "def all_keypoints_on_scene(keypoints_output_scene, scene_keypoints, close_matches):\n",
    "\n",
    "    keypoints_output_scene = cv2.drawKeypoints(keypoints_output_scene, scene_keypoints, None, color=(0, 0, 255))\n",
    "\n",
    "    for match in close_matches:\n",
    "        keypoints_output_scene = cv2.circle(keypoints_output_scene, (int(scene_keypoints[match[0].trainIdx].pt[0]), int(scene_keypoints[match[0].trainIdx].pt[1])), 10, (255, 0, 0), 3)\n",
    "            \n",
    "    return keypoints_output_scene\n",
    "\n",
    "\n",
    "#this method needs to be separate as we dont want to do this everytime in the above code.\n",
    "def all_keypoints_on_objects(objects):\n",
    "\n",
    "    object_keypoints = []\n",
    "    object_descriptors = []\n",
    "\n",
    "    for current_object in objects:\n",
    "        sift_instance = cv2.SIFT_create()\n",
    "\n",
    "        train_object = current_object[\"image\"]\n",
    "        keypoint, descriptors = sift_instance.detectAndCompute(train_object, None)\n",
    "        copy_obj = train_object.copy()\n",
    "\n",
    "        object_keypoints.append(keypoint)\n",
    "        object_descriptors.append(descriptors)\n",
    "\n",
    "        copy_obj = cv2.drawKeypoints(copy_obj, keypoint, None, color=(0, 0, 255))\n",
    "        copy_obj = cv2.cvtColor(copy_obj, cv2.COLOR_BGR2RGB)\n",
    "        cv2.imwrite(f\"../Keypoints/{current_object['id']}_keypoints.png\", copy_obj)\n",
    " \n",
    "    return object_keypoints, object_descriptors\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Shows the keypoints for every object individually on each scene. Each scene has multiple outputs to show keypoints for each object.\n",
    "The object is shown on the right of each scene to denote the keypoints being shown in scene.\n",
    "\"\"\"\n",
    "#this one we are just saving every scene with keypoints on each specific object.\n",
    "def matching(key, raw_scene, curr_obj, scene_keypoints, close_matches):\n",
    "    \n",
    "    curr_obj_image = curr_obj[\"image\"]\n",
    "            \n",
    "    for match in close_matches:\n",
    "        cv2.circle(raw_scene, (int(scene_keypoints[match[0].trainIdx].pt[0]), int(scene_keypoints[match[0].trainIdx].pt[1])), 10, (255, 0, 0), 3)\n",
    "\n",
    "    curr_obj_image = cv2.resize(curr_obj_image, (0, 0), fx=0.5, fy=0.5)\n",
    "    \n",
    "    height = max(raw_scene.shape[0], curr_obj_image.shape[0])\n",
    "    width = raw_scene.shape[1] + curr_obj_image.shape[1]\n",
    "    \n",
    "    combined_image = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    \n",
    "    combined_image[:raw_scene.shape[0], :raw_scene.shape[1]] = raw_scene\n",
    "    combined_image[:curr_obj_image.shape[0], raw_scene.shape[1]:] = curr_obj_image\n",
    "    \n",
    "    # fix color before saving\n",
    "    combined_image = cv2.cvtColor(combined_image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    cv2.imwrite(f\"../Matches/{key}_{curr_obj['id']}_matches.png\", combined_image)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "#get the keypoints and the object descriptors for all objects to increase performance\n",
    "object_keypoints, object_descriptors = all_keypoints_on_objects(objects)\n",
    "\n",
    "#run all instances\n",
    "result = []\n",
    "for key in scenes:\n",
    "    result.append(summarize_images(key[:-4], scenes[key], objects, object_keypoints, object_descriptors)) #key[:-4] for file naming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Stitching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stitcher object from OpenCV\n",
    "stitcher = Stitcher.create()\n",
    "stitched_images = {}\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "# go through all of the scene groups to stitch\n",
    "for key in groups:\n",
    "  # get the images\n",
    "  images = groups[key]\n",
    "  \n",
    "  if len(images) > 1:\n",
    "    # put the images together with the stitcher\n",
    "    _, result = stitcher.stitch(images)\n",
    "  else:\n",
    "    result = images[0]\n",
    "  \n",
    "  if result is None:\n",
    "    print(f\"Could not stitch {key}\")\n",
    "    continue\n",
    "    \n",
    "  # pass result through boxes function to detect objects\n",
    "  result = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)\n",
    "  \n",
    "  # store the stitched image\n",
    "  stitched_images[key] = result\n",
    "  \n",
    "  # save & plot\n",
    "  cv2.imwrite(f\"../Stitched/{key}.png\", result)\n",
    "  plt.subplot(1, len(groups), int(key[-1]))\n",
    "  plt.imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n",
    "  plt.axis(\"off\")\n",
    "  plt.title(key)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stitching_boxes(key, scene, objects, object_keypoints, object_descriptors, min_matches = 28, threshold = 0.7):\n",
    "\n",
    "    #The boxed and scenes containing all of the keypoints need to survive on each iteration:\n",
    "    boxes_output_scene = scene.copy()\n",
    "\n",
    "    for i, curr_obj in enumerate(objects):\n",
    "        raw_scene = scene.copy() #for the matching output (need raw scene everytime to match individual objects)\n",
    "        sift_instance = cv2.SIFT_create()\n",
    "        \n",
    "        obj_keypoints = object_keypoints[i]\n",
    "        obj_descriptors = object_descriptors[i]\n",
    "        scene_keypoints, scene_descriptors = sift_instance.detectAndCompute(raw_scene, None)\n",
    "        \n",
    "        #https://docs.opencv.org/3.4/dc/dc3/tutorial_py_matcher.html\n",
    "        brute_force_matcher = cv2.BFMatcher()\n",
    "        matches = brute_force_matcher.knnMatch(obj_descriptors, scene_descriptors, k=2)\n",
    "\n",
    "        #Check to see what matches out of them are within the predefined threshold (overall closeness)\n",
    "        close_matches = []\n",
    "        close_matches_nonlist = []\n",
    "\n",
    "        for m, n in matches:\n",
    "            if m.distance < threshold*n.distance:\n",
    "                close_matches.append([m])\n",
    "                close_matches_nonlist.append(m)\n",
    "\n",
    "        if len(close_matches) >= min_matches:\n",
    "             \n",
    "            #decrease the total number of matches to optimize the runtime below\n",
    "            close_matches = close_matches[:80]\n",
    "            close_matches_nonlist = close_matches_nonlist[:80]\n",
    "\n",
    "            boxes_output_scene = boxes(boxes_output_scene, curr_obj, obj_keypoints, scene_keypoints, close_matches_nonlist)\n",
    "    \n",
    "    cv2.imwrite(f\"../Stitched/{key}_detected.png\", boxes_output_scene)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def boxes(boxes_output_scene, curr_obj, obj_keypoints, scene_keypoints, close_matches_nonlist):\n",
    "\n",
    "    # https://docs.opencv.org/3.4/d1/de0/tutorial_py_feature_homography.html\n",
    "\n",
    "    curr_obj_image = curr_obj[\"image\"]\n",
    "\n",
    "    # draw box around detected image\n",
    "    object_points = np.float32([ obj_keypoints[m.queryIdx].pt for m in close_matches_nonlist ]).reshape(-1, 1, 2)\n",
    "    scene_points = np.float32([ scene_keypoints[m.trainIdx].pt for m in close_matches_nonlist ]).reshape(-1, 1, 2)\n",
    "    mask, _ = cv2.findHomography(object_points, scene_points, cv2.RANSAC, 5.0)\n",
    "    height, width, _ = curr_obj_image.shape\n",
    "    points = np.float32([ [0, 0], [0, height-1], [width-1, height-1], [width-1, 0] ]).reshape(-1, 1, 2)\n",
    "    scene = cv2.perspectiveTransform(points, mask)\n",
    "    boxes_output_scene = cv2.polylines(boxes_output_scene,[np.int32(scene)], True, 255, 3, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "    # label all of the scene objects\n",
    "    # https://docs.opencv.org/4.x/d6/d6e/group__imgproc__draw.html#ga5126f47f883d730f633d74f07456c576\n",
    "    cv2.putText(img=boxes_output_scene, text=curr_obj['name'], org=(int(scene[0][0][0]), int(scene[1][0][1])), fontFace=cv2.FONT_HERSHEY_DUPLEX, fontScale=3, color=(0, 0, 0), thickness=4, lineType=cv2.FILLED, bottomLeftOrigin = False)\n",
    "    \n",
    "    return boxes_output_scene\n",
    "\n",
    "\n",
    "#Run to detect the objects in each of the stitched scenes\n",
    "for key in stitched_images:\n",
    "    stitching_boxes(key, stitched_images[key], objects, object_keypoints, object_descriptors)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
